{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ”„ Data Preprocessing & Augmentation\n",
                "\n",
                "This notebook preprocesses the collected hand landmark data and prepares it for model training.\n",
                "\n",
                "## Objectives\n",
                "- Load collected landmark data\n",
                "- Explore and visualize the dataset\n",
                "- Normalize and standardize features\n",
                "- Apply data augmentation techniques\n",
                "- Encode labels\n",
                "- Split data into train/validation/test sets\n",
                "- Save processed data\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.utils import shuffle\n",
                "import pickle\n",
                "import json\n",
                "from datetime import datetime\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random seed\n",
                "np.random.seed(42)\n",
                "\n",
                "print(\"âœ… Libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Raw Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "DATA_DIR = 'data/raw'\n",
                "PROCESSED_DIR = 'data/processed'\n",
                "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
                "\n",
                "# Load landmarks and labels\n",
                "X = np.load(os.path.join(DATA_DIR, 'landmarks.npy'))\n",
                "y = np.load(os.path.join(DATA_DIR, 'labels.npy'))\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"RAW DATA LOADED\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Data shape: {X.shape}\")\n",
                "print(f\"Labels shape: {y.shape}\")\n",
                "print(f\"Number of features: {X.shape[1]}\")\n",
                "print(f\"Number of samples: {X.shape[0]}\")\n",
                "print(f\"Unique classes: {np.unique(y)}\")\n",
                "print(f\"Number of classes: {len(np.unique(y))}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load metadata if available\n",
                "metadata_path = os.path.join(DATA_DIR, 'metadata.json')\n",
                "if os.path.exists(metadata_path):\n",
                "    with open(metadata_path, 'r') as f:\n",
                "        metadata = json.load(f)\n",
                "    print(\"\\nMetadata:\")\n",
                "    print(json.dumps(metadata, indent=2))\n",
                "else:\n",
                "    print(\"\\nâš ï¸ No metadata file found\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Exploratory Data Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Class distribution\n",
                "unique, counts = np.unique(y, return_counts=True)\n",
                "class_distribution = dict(zip(unique, counts))\n",
                "\n",
                "print(\"\\nClass Distribution:\")\n",
                "print(\"=\"*40)\n",
                "for class_name, count in sorted(class_distribution.items()):\n",
                "    percentage = (count / len(y)) * 100\n",
                "    print(f\"  {class_name:10s}: {count:4d} ({percentage:5.1f}%)\")\n",
                "print(\"=\"*40)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize class distribution\n",
                "plt.figure(figsize=(12, 6))\n",
                "bars = plt.bar(unique, counts, color='steelblue', edgecolor='navy', linewidth=1.5)\n",
                "\n",
                "# Add value labels\n",
                "for bar in bars:\n",
                "    height = bar.get_height()\n",
                "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
                "            f'{int(height)}',\n",
                "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
                "\n",
                "plt.xlabel('Class', fontsize=12, fontweight='bold')\n",
                "plt.ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
                "plt.title('Class Distribution - Raw Data', fontsize=14, fontweight='bold')\n",
                "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
                "plt.tight_layout()\n",
                "plt.savefig('outputs/visualizations/class_distribution_raw.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature statistics\n",
                "print(\"\\nFeature Statistics:\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Min value: {X.min():.6f}\")\n",
                "print(f\"Max value: {X.max():.6f}\")\n",
                "print(f\"Mean value: {X.mean():.6f}\")\n",
                "print(f\"Std deviation: {X.std():.6f}\")\n",
                "print(f\"Median value: {np.median(X):.6f}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize feature distributions\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# Sample a few features to visualize\n",
                "sample_features = [0, 1, 2, 20]  # x, y, z of first landmark, and x of 7th landmark\n",
                "feature_names = ['Landmark 0 - X', 'Landmark 0 - Y', 'Landmark 0 - Z', 'Landmark 7 - X']\n",
                "\n",
                "for idx, (ax, feat_idx, feat_name) in enumerate(zip(axes.flat, sample_features, feature_names)):\n",
                "    ax.hist(X[:, feat_idx], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
                "    ax.set_xlabel('Value', fontsize=10)\n",
                "    ax.set_ylabel('Frequency', fontsize=10)\n",
                "    ax.set_title(feat_name, fontsize=11, fontweight='bold')\n",
                "    ax.grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('outputs/visualizations/feature_distributions.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Data Cleaning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "print(\"\\nData Quality Check:\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "nan_count = np.isnan(X).sum()\n",
                "inf_count = np.isinf(X).sum()\n",
                "\n",
                "print(f\"NaN values: {nan_count}\")\n",
                "print(f\"Inf values: {inf_count}\")\n",
                "\n",
                "if nan_count > 0 or inf_count > 0:\n",
                "    print(\"\\nâš ï¸ Cleaning data...\")\n",
                "    # Remove rows with NaN or Inf\n",
                "    valid_indices = ~(np.isnan(X).any(axis=1) | np.isinf(X).any(axis=1))\n",
                "    X = X[valid_indices]\n",
                "    y = y[valid_indices]\n",
                "    print(f\"âœ… Removed {(~valid_indices).sum()} invalid samples\")\n",
                "    print(f\"   Remaining samples: {X.shape[0]}\")\n",
                "else:\n",
                "    print(\"âœ… No missing or infinite values found\")\n",
                "\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Label Encoding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode labels\n",
                "label_encoder = LabelEncoder()\n",
                "y_encoded = label_encoder.fit_transform(y)\n",
                "\n",
                "print(\"\\nLabel Encoding:\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Original labels: {label_encoder.classes_}\")\n",
                "print(f\"Encoded range: 0 to {y_encoded.max()}\")\n",
                "print(\"\\nMapping:\")\n",
                "for idx, class_name in enumerate(label_encoder.classes_):\n",
                "    print(f\"  {class_name} -> {idx}\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Save label encoder\n",
                "os.makedirs('models', exist_ok=True)\n",
                "with open('models/label_encoder.pkl', 'wb') as f:\n",
                "    pickle.dump(label_encoder, f)\n",
                "print(\"\\nâœ… Label encoder saved to models/label_encoder.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Feature Normalization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Normalize features using StandardScaler\n",
                "scaler = StandardScaler()\n",
                "X_normalized = scaler.fit_transform(X)\n",
                "\n",
                "print(\"\\nFeature Normalization:\")\n",
                "print(\"=\"*60)\n",
                "print(\"Before normalization:\")\n",
                "print(f\"  Mean: {X.mean():.6f}\")\n",
                "print(f\"  Std:  {X.std():.6f}\")\n",
                "print(f\"  Min:  {X.min():.6f}\")\n",
                "print(f\"  Max:  {X.max():.6f}\")\n",
                "\n",
                "print(\"\\nAfter normalization:\")\n",
                "print(f\"  Mean: {X_normalized.mean():.6f}\")\n",
                "print(f\"  Std:  {X_normalized.std():.6f}\")\n",
                "print(f\"  Min:  {X_normalized.min():.6f}\")\n",
                "print(f\"  Max:  {X_normalized.max():.6f}\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Save scaler\n",
                "with open('models/scaler.pkl', 'wb') as f:\n",
                "    pickle.dump(scaler, f)\n",
                "print(\"\\nâœ… Scaler saved to models/scaler.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Data Augmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def augment_data(X, y, augmentation_factor=2):\n",
                "    \"\"\"\n",
                "    Augment hand landmark data with noise and transformations.\n",
                "    \n",
                "    Args:\n",
                "        X: Feature array\n",
                "        y: Label array\n",
                "        augmentation_factor: Number of augmented copies per sample\n",
                "    \n",
                "    Returns:\n",
                "        Augmented X and y arrays\n",
                "    \"\"\"\n",
                "    X_augmented = [X]\n",
                "    y_augmented = [y]\n",
                "    \n",
                "    for i in range(augmentation_factor):\n",
                "        # Add Gaussian noise\n",
                "        noise = np.random.normal(0, 0.02, X.shape)\n",
                "        X_noisy = X + noise\n",
                "        \n",
                "        # Random scaling\n",
                "        scale_factor = np.random.uniform(0.95, 1.05)\n",
                "        X_scaled = X * scale_factor\n",
                "        \n",
                "        X_augmented.extend([X_noisy, X_scaled])\n",
                "        y_augmented.extend([y, y])\n",
                "    \n",
                "    return np.vstack(X_augmented), np.concatenate(y_augmented)\n",
                "\n",
                "# Apply augmentation\n",
                "print(\"\\nData Augmentation:\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Original dataset size: {X_normalized.shape[0]}\")\n",
                "\n",
                "X_augmented, y_augmented = augment_data(X_normalized, y_encoded, augmentation_factor=1)\n",
                "\n",
                "print(f\"Augmented dataset size: {X_augmented.shape[0]}\")\n",
                "print(f\"Augmentation ratio: {X_augmented.shape[0] / X_normalized.shape[0]:.1f}x\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Train/Validation/Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Shuffle data\n",
                "X_augmented, y_augmented = shuffle(X_augmented, y_augmented, random_state=42)\n",
                "\n",
                "# Split: 70% train, 15% validation, 15% test\n",
                "X_train, X_temp, y_train, y_temp = train_test_split(\n",
                "    X_augmented, y_augmented, \n",
                "    test_size=0.3, \n",
                "    random_state=42, \n",
                "    stratify=y_augmented\n",
                ")\n",
                "\n",
                "X_val, X_test, y_val, y_test = train_test_split(\n",
                "    X_temp, y_temp, \n",
                "    test_size=0.5, \n",
                "    random_state=42, \n",
                "    stratify=y_temp\n",
                ")\n",
                "\n",
                "print(\"\\nData Split:\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Training set:   {X_train.shape[0]:5d} samples ({X_train.shape[0]/X_augmented.shape[0]*100:.1f}%)\")\n",
                "print(f\"Validation set: {X_val.shape[0]:5d} samples ({X_val.shape[0]/X_augmented.shape[0]*100:.1f}%)\")\n",
                "print(f\"Test set:       {X_test.shape[0]:5d} samples ({X_test.shape[0]/X_augmented.shape[0]*100:.1f}%)\")\n",
                "print(f\"Total:          {X_augmented.shape[0]:5d} samples\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify stratification\n",
                "print(\"\\nClass Distribution Verification:\")\n",
                "print(\"=\"*60)\n",
                "print(f\"{'Class':<10} {'Train':<10} {'Val':<10} {'Test':<10}\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "for class_idx, class_name in enumerate(label_encoder.classes_):\n",
                "    train_count = (y_train == class_idx).sum()\n",
                "    val_count = (y_val == class_idx).sum()\n",
                "    test_count = (y_test == class_idx).sum()\n",
                "    print(f\"{class_name:<10} {train_count:<10} {val_count:<10} {test_count:<10}\")\n",
                "\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Save Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save processed data\n",
                "print(\"\\nSaving processed data...\")\n",
                "\n",
                "np.save(os.path.join(PROCESSED_DIR, 'X_train.npy'), X_train)\n",
                "np.save(os.path.join(PROCESSED_DIR, 'X_val.npy'), X_val)\n",
                "np.save(os.path.join(PROCESSED_DIR, 'X_test.npy'), X_test)\n",
                "np.save(os.path.join(PROCESSED_DIR, 'y_train.npy'), y_train)\n",
                "np.save(os.path.join(PROCESSED_DIR, 'y_val.npy'), y_val)\n",
                "np.save(os.path.join(PROCESSED_DIR, 'y_test.npy'), y_test)\n",
                "\n",
                "print(\"âœ… Processed data saved to data/processed/\")\n",
                "print(\"\\nFiles created:\")\n",
                "print(\"  - X_train.npy, y_train.npy\")\n",
                "print(\"  - X_val.npy, y_val.npy\")\n",
                "print(\"  - X_test.npy, y_test.npy\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Create Processing Metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create preprocessing metadata\n",
                "preprocessing_metadata = {\n",
                "    'preprocessing_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
                "    'original_samples': X.shape[0],\n",
                "    'augmented_samples': X_augmented.shape[0],\n",
                "    'train_samples': X_train.shape[0],\n",
                "    'val_samples': X_val.shape[0],\n",
                "    'test_samples': X_test.shape[0],\n",
                "    'num_features': X_train.shape[1],\n",
                "    'num_classes': len(label_encoder.classes_),\n",
                "    'classes': label_encoder.classes_.tolist(),\n",
                "    'normalization': 'StandardScaler',\n",
                "    'augmentation_factor': X_augmented.shape[0] / X.shape[0],\n",
                "    'train_split': 0.7,\n",
                "    'val_split': 0.15,\n",
                "    'test_split': 0.15,\n",
                "    'random_seed': 42\n",
                "}\n",
                "\n",
                "# Save metadata\n",
                "with open(os.path.join(PROCESSED_DIR, 'preprocessing_metadata.json'), 'w') as f:\n",
                "    json.dump(preprocessing_metadata, f, indent=4)\n",
                "\n",
                "print(\"\\nâœ… Preprocessing metadata saved!\")\n",
                "print(\"\\nMetadata:\")\n",
                "print(json.dumps(preprocessing_metadata, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Summary Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create summary visualization\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Dataset sizes\n",
                "sizes = [X_train.shape[0], X_val.shape[0], X_test.shape[0]]\n",
                "labels_pie = ['Train (70%)', 'Validation (15%)', 'Test (15%)']\n",
                "colors = ['#66b3ff', '#99ff99', '#ffcc99']\n",
                "\n",
                "axes[0].pie(sizes, labels=labels_pie, colors=colors, autopct='%1.1f%%',\n",
                "           startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
                "axes[0].set_title('Data Split Distribution', fontsize=13, fontweight='bold')\n",
                "\n",
                "# Class distribution in training set\n",
                "train_class_counts = []\n",
                "for class_idx in range(len(label_encoder.classes_)):\n",
                "    train_class_counts.append((y_train == class_idx).sum())\n",
                "\n",
                "axes[1].bar(label_encoder.classes_, train_class_counts, \n",
                "           color='steelblue', edgecolor='navy', linewidth=1.5)\n",
                "axes[1].set_xlabel('Class', fontsize=11, fontweight='bold')\n",
                "axes[1].set_ylabel('Number of Samples', fontsize=11, fontweight='bold')\n",
                "axes[1].set_title('Training Set Class Distribution', fontsize=13, fontweight='bold')\n",
                "axes[1].grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('outputs/visualizations/preprocessing_summary.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸŽ¯ Summary\n",
                "\n",
                "Data preprocessing completed successfully!\n",
                "\n",
                "### What Was Done:\n",
                "- âœ… Loaded and cleaned raw data\n",
                "- âœ… Encoded labels and saved encoder\n",
                "- âœ… Normalized features using StandardScaler\n",
                "- âœ… Applied data augmentation\n",
                "- âœ… Split data into train/val/test sets (70/15/15)\n",
                "- âœ… Saved processed data and metadata\n",
                "\n",
                "### What's Next?\n",
                "Proceed to **04_model_training.ipynb** to:\n",
                "- Build the deep learning model architecture\n",
                "- Train the model on processed data\n",
                "- Monitor training with callbacks\n",
                "- Save the best model\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}