{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé• Real-Time Sign Language Detection\n",
                "\n",
                "This notebook demonstrates real-time sign language translation using your webcam and the trained model.\n",
                "\n",
                "## Objectives\n",
                "- Load trained model and preprocessing tools\n",
                "- Initialize webcam and MediaPipe\n",
                "- Perform real-time hand detection and classification\n",
                "- Display predictions with confidence scores\n",
                "- Test model in real-world conditions\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import cv2\n",
                "import mediapipe as mp\n",
                "import pickle\n",
                "from datetime import datetime\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# TensorFlow\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "\n",
                "print(\"‚úÖ Libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model and Preprocessing Tools"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load trained model\n",
                "MODELS_DIR = 'models/saved_models'\n",
                "model = keras.models.load_model(os.path.join(MODELS_DIR, 'best_model.keras'))\n",
                "\n",
                "print(\"‚úÖ Model loaded successfully\")\n",
                "print(f\"   Model: {model.name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load label encoder\n",
                "with open('models/label_encoder.pkl', 'rb') as f:\n",
                "    label_encoder = pickle.load(f)\n",
                "\n",
                "# Load scaler\n",
                "with open('models/scaler.pkl', 'rb') as f:\n",
                "    scaler = pickle.load(f)\n",
                "\n",
                "class_names = label_encoder.classes_\n",
                "\n",
                "print(\"\\n‚úÖ Preprocessing tools loaded\")\n",
                "print(f\"   Classes: {class_names}\")\n",
                "print(f\"   Number of classes: {len(class_names)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Initialize MediaPipe"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize MediaPipe Hands\n",
                "mp_hands = mp.solutions.hands\n",
                "mp_drawing = mp.solutions.drawing_utils\n",
                "mp_drawing_styles = mp.solutions.drawing_styles\n",
                "\n",
                "print(\"‚úÖ MediaPipe initialized\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Real-Time Detection Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def realtime_detection(\n",
                "    model, \n",
                "    label_encoder, \n",
                "    scaler,\n",
                "    confidence_threshold=0.7,\n",
                "    camera_index=0\n",
                "):\n",
                "    \"\"\"\n",
                "    Run real-time sign language detection using webcam.\n",
                "    \n",
                "    Args:\n",
                "        model: Trained Keras model\n",
                "        label_encoder: Fitted LabelEncoder\n",
                "        scaler: Fitted StandardScaler\n",
                "        confidence_threshold: Minimum confidence for display (0-1)\n",
                "        camera_index: Camera device index\n",
                "    \n",
                "    Controls:\n",
                "        ESC - Exit\n",
                "        SPACE - Pause/Resume\n",
                "        S - Save screenshot\n",
                "    \"\"\"\n",
                "    cap = cv2.VideoCapture(camera_index)\n",
                "    \n",
                "    # Set camera properties\n",
                "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
                "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
                "    cap.set(cv2.CAP_PROP_FPS, 30)\n",
                "    \n",
                "    # Initialize MediaPipe Hands\n",
                "    with mp_hands.Hands(\n",
                "        static_image_mode=False,\n",
                "        max_num_hands=1,\n",
                "        min_detection_confidence=0.7,\n",
                "        min_tracking_confidence=0.7\n",
                "    ) as hands:\n",
                "        \n",
                "        print(\"\\n\" + \"=\"*60)\n",
                "        print(\"REAL-TIME SIGN LANGUAGE DETECTION\")\n",
                "        print(\"=\"*60)\n",
                "        print(\"Controls:\")\n",
                "        print(\"  ESC   - Exit\")\n",
                "        print(\"  SPACE - Pause/Resume\")\n",
                "        print(\"  S     - Save screenshot\")\n",
                "        print(\"=\"*60 + \"\\n\")\n",
                "        \n",
                "        paused = False\n",
                "        frame_count = 0\n",
                "        fps_start_time = datetime.now()\n",
                "        fps = 0\n",
                "        \n",
                "        # For smoothing predictions\n",
                "        prediction_history = []\n",
                "        history_size = 5\n",
                "        \n",
                "        while cap.isOpened():\n",
                "            ret, frame = cap.read()\n",
                "            if not ret:\n",
                "                print(\"‚ùå Error: Cannot read from webcam\")\n",
                "                break\n",
                "            \n",
                "            frame = cv2.flip(frame, 1)  # Mirror image\n",
                "            h, w, _ = frame.shape\n",
                "            \n",
                "            if not paused:\n",
                "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
                "                results = hands.process(rgb_frame)\n",
                "                \n",
                "                # Default display\n",
                "                display_text = \"No hand detected\"\n",
                "                display_confidence = 0.0\n",
                "                display_color = (0, 0, 255)  # Red\n",
                "                \n",
                "                if results.multi_hand_landmarks:\n",
                "                    for hand_landmarks in results.multi_hand_landmarks:\n",
                "                        # Draw landmarks\n",
                "                        mp_drawing.draw_landmarks(\n",
                "                            frame,\n",
                "                            hand_landmarks,\n",
                "                            mp_hands.HAND_CONNECTIONS,\n",
                "                            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
                "                            mp_drawing_styles.get_default_hand_connections_style()\n",
                "                        )\n",
                "                        \n",
                "                        # Extract landmarks\n",
                "                        landmarks = []\n",
                "                        for lm in hand_landmarks.landmark:\n",
                "                            landmarks.extend([lm.x, lm.y, lm.z])\n",
                "                        \n",
                "                        # Preprocess and predict\n",
                "                        landmarks_array = np.array(landmarks).reshape(1, -1)\n",
                "                        landmarks_scaled = scaler.transform(landmarks_array)\n",
                "                        \n",
                "                        prediction = model.predict(landmarks_scaled, verbose=0)\n",
                "                        predicted_class = np.argmax(prediction)\n",
                "                        confidence = prediction[0][predicted_class]\n",
                "                        \n",
                "                        # Add to history for smoothing\n",
                "                        prediction_history.append(predicted_class)\n",
                "                        if len(prediction_history) > history_size:\n",
                "                            prediction_history.pop(0)\n",
                "                        \n",
                "                        # Use most common prediction in history\n",
                "                        if len(prediction_history) >= 3:\n",
                "                            from collections import Counter\n",
                "                            smoothed_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
                "                        else:\n",
                "                            smoothed_prediction = predicted_class\n",
                "                        \n",
                "                        # Get class name\n",
                "                        class_name = label_encoder.inverse_transform([smoothed_prediction])[0]\n",
                "                        \n",
                "                        # Update display if confidence is high enough\n",
                "                        if confidence >= confidence_threshold:\n",
                "                            display_text = f\"Sign: {class_name}\"\n",
                "                            display_confidence = confidence\n",
                "                            display_color = (0, 255, 0)  # Green\n",
                "                        else:\n",
                "                            display_text = f\"Low confidence ({confidence*100:.1f}%)\"\n",
                "                            display_color = (0, 165, 255)  # Orange\n",
                "                \n",
                "                # Calculate FPS\n",
                "                frame_count += 1\n",
                "                if frame_count % 30 == 0:\n",
                "                    elapsed_time = (datetime.now() - fps_start_time).total_seconds()\n",
                "                    fps = 30 / elapsed_time if elapsed_time > 0 else 0\n",
                "                    fps_start_time = datetime.now()\n",
                "            \n",
                "            # Create overlay panel\n",
                "            overlay = frame.copy()\n",
                "            cv2.rectangle(overlay, (0, 0), (w, 150), (0, 0, 0), -1)\n",
                "            frame = cv2.addWeighted(overlay, 0.6, frame, 0.4, 0)\n",
                "            \n",
                "            # Display prediction\n",
                "            cv2.putText(frame, display_text, (20, 60), \n",
                "                       cv2.FONT_HERSHEY_SIMPLEX, 1.5, display_color, 3)\n",
                "            \n",
                "            if display_confidence > 0:\n",
                "                cv2.putText(frame, f\"Confidence: {display_confidence*100:.1f}%\", (20, 110), \n",
                "                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
                "            \n",
                "            # Display FPS\n",
                "            cv2.putText(frame, f\"FPS: {fps:.1f}\", (w - 150, 40), \n",
                "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
                "            \n",
                "            # Display status\n",
                "            if paused:\n",
                "                cv2.putText(frame, \"PAUSED\", (w - 200, h - 30), \n",
                "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 3)\n",
                "            \n",
                "            # Display controls\n",
                "            cv2.putText(frame, \"ESC: Exit | SPACE: Pause | S: Screenshot\", (20, h - 20), \n",
                "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)\n",
                "            \n",
                "            cv2.imshow('Sign Language Translator', frame)\n",
                "            \n",
                "            # Handle key presses\n",
                "            key = cv2.waitKey(1) & 0xFF\n",
                "            \n",
                "            if key == 27:  # ESC\n",
                "                print(\"\\n‚èπÔ∏è  Detection stopped by user\")\n",
                "                break\n",
                "            elif key == 32:  # SPACE\n",
                "                paused = not paused\n",
                "                print(f\"\\n{'‚è∏Ô∏è  Paused' if paused else '‚ñ∂Ô∏è  Resumed'}\")\n",
                "            elif key == ord('s') or key == ord('S'):  # S\n",
                "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "                filename = f\"outputs/visualizations/screenshot_{timestamp}.png\"\n",
                "                cv2.imwrite(filename, frame)\n",
                "                print(f\"\\nüì∏ Screenshot saved: {filename}\")\n",
                "    \n",
                "    cap.release()\n",
                "    cv2.destroyAllWindows()\n",
                "    \n",
                "    print(\"\\n‚úÖ Detection session ended\")\n",
                "    print(f\"   Total frames processed: {frame_count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Run Real-Time Detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "CONFIDENCE_THRESHOLD = 0.7  # Adjust this value (0.0 to 1.0)\n",
                "CAMERA_INDEX = 0  # Change if you have multiple cameras\n",
                "\n",
                "print(\"\\nConfiguration:\")\n",
                "print(f\"  Confidence threshold: {CONFIDENCE_THRESHOLD}\")\n",
                "print(f\"  Camera index: {CAMERA_INDEX}\")\n",
                "print(f\"  Classes: {class_names}\")\n",
                "print(\"\\nStarting real-time detection...\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run detection\n",
                "# Uncomment the line below to start\n",
                "realtime_detection(model, label_encoder, scaler, CONFIDENCE_THRESHOLD, CAMERA_INDEX)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Tips for Better Real-Time Performance"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Improving Accuracy:\n",
                "1. **Lighting**: Ensure good, even lighting on your hand\n",
                "2. **Background**: Use a plain, contrasting background\n",
                "3. **Hand Position**: Keep your hand centered and fully visible\n",
                "4. **Gesture Consistency**: Form gestures exactly as during training\n",
                "5. **Distance**: Maintain consistent distance from camera\n",
                "\n",
                "### Adjusting Parameters:\n",
                "- **Confidence Threshold**: Lower for more predictions, higher for more accuracy\n",
                "- **Detection Confidence**: Adjust in MediaPipe initialization (0.5-0.9)\n",
                "- **Tracking Confidence**: Adjust for smoother tracking (0.5-0.9)\n",
                "- **History Size**: Increase for smoother predictions, decrease for faster response\n",
                "\n",
                "### Troubleshooting:\n",
                "- **No hand detected**: Check lighting and hand visibility\n",
                "- **Wrong predictions**: Retrain with more diverse data\n",
                "- **Low FPS**: Reduce frame resolution or use GPU\n",
                "- **Jittery predictions**: Increase history size for smoothing"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Advanced: Record Detection Session"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def record_detection_session(model, label_encoder, scaler, output_filename='detection_session.avi'):\n",
                "    \"\"\"\n",
                "    Record a detection session to video file.\n",
                "    \"\"\"\n",
                "    cap = cv2.VideoCapture(0)\n",
                "    \n",
                "    # Get frame properties\n",
                "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
                "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
                "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
                "    \n",
                "    # Define codec and create VideoWriter\n",
                "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
                "    out = cv2.VideoWriter(output_filename, fourcc, fps, (frame_width, frame_height))\n",
                "    \n",
                "    print(f\"\\nüé¨ Recording to: {output_filename}\")\n",
                "    print(\"Press ESC to stop recording\\n\")\n",
                "    \n",
                "    with mp_hands.Hands(\n",
                "        static_image_mode=False,\n",
                "        max_num_hands=1,\n",
                "        min_detection_confidence=0.7,\n",
                "        min_tracking_confidence=0.7\n",
                "    ) as hands:\n",
                "        \n",
                "        while cap.isOpened():\n",
                "            ret, frame = cap.read()\n",
                "            if not ret:\n",
                "                break\n",
                "            \n",
                "            frame = cv2.flip(frame, 1)\n",
                "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
                "            results = hands.process(rgb_frame)\n",
                "            \n",
                "            # Process and display (similar to realtime_detection)\n",
                "            # ... (add processing code here)\n",
                "            \n",
                "            # Write frame to video\n",
                "            out.write(frame)\n",
                "            \n",
                "            cv2.imshow('Recording...', frame)\n",
                "            \n",
                "            if cv2.waitKey(1) & 0xFF == 27:  # ESC\n",
                "                break\n",
                "    \n",
                "    cap.release()\n",
                "    out.release()\n",
                "    cv2.destroyAllWindows()\n",
                "    \n",
                "    print(f\"\\n‚úÖ Recording saved: {output_filename}\")\n",
                "\n",
                "# Uncomment to record a session\n",
                "# record_detection_session(model, label_encoder, scaler, 'outputs/visualizations/demo_session.avi')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéØ Summary\n",
                "\n",
                "Real-time detection setup complete!\n",
                "\n",
                "### What Was Done:\n",
                "- ‚úÖ Loaded trained model and preprocessing tools\n",
                "- ‚úÖ Initialized MediaPipe for hand tracking\n",
                "- ‚úÖ Created real-time detection function\n",
                "- ‚úÖ Added prediction smoothing\n",
                "- ‚úÖ Implemented interactive controls\n",
                "- ‚úÖ Added recording capability\n",
                "\n",
                "### Features:\n",
                "- Real-time hand landmark detection\n",
                "- Sign language classification with confidence scores\n",
                "- FPS monitoring\n",
                "- Prediction smoothing for stability\n",
                "- Screenshot capture\n",
                "- Pause/resume functionality\n",
                "\n",
                "### Next Steps:\n",
                "1. Test with different lighting conditions\n",
                "2. Collect more data for underperforming classes\n",
                "3. Expand to more sign language gestures\n",
                "4. Add sentence construction from word sequences\n",
                "5. Integrate text-to-speech for accessibility\n",
                "6. Deploy as a web or mobile application\n",
                "\n",
                "---\n",
                "\n",
                "## üéâ Congratulations!\n",
                "\n",
                "You've successfully built a complete real-time sign language translator!\n",
                "\n",
                "This project demonstrates:\n",
                "- Computer vision with MediaPipe\n",
                "- Deep learning with TensorFlow/Keras\n",
                "- Real-time inference\n",
                "- End-to-end ML pipeline\n",
                "\n",
                "**Keep improving and expanding your model!** üöÄ\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}