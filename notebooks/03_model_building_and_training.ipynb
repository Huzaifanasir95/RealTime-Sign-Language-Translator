{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8580d5d",
   "metadata": {},
   "source": [
    "# Real-Time Sign Language Translator - Model Building & Training\n",
    "\n",
    "This notebook builds and trains a CNN model for sign language recognition.\n",
    "\n",
    "**Objectives:**\n",
    "- Build CNN architecture (ResNet-based)\n",
    "- Define loss function and optimizer\n",
    "- Implement training loop with validation\n",
    "- Track metrics and visualize training progress\n",
    "- Save best model checkpoints\n",
    "- Evaluate model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a015f9f",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee29e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration loaded successfully!\n",
      "Device: cuda\n",
      "Number of classes: 26\n",
      "Project root: d:\\Projects\\RealTime-Sign-Language-Translator\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load configuration\n",
    "project_root = os.path.abspath('..')\n",
    "config_file = os.path.join(project_root, 'config.json')\n",
    "\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Load class mapping\n",
    "class_mapping_file = os.path.join(project_root, 'class_mapping.json')\n",
    "with open(class_mapping_file, 'r') as f:\n",
    "    class_mapping = json.load(f)\n",
    "    class_to_idx = class_mapping['class_to_idx']\n",
    "    idx_to_class = {int(k): v for k, v in class_mapping['idx_to_class'].items()}\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config['device'] = device\n",
    "\n",
    "print(\"‚úì Configuration loaded successfully!\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Number of classes: {len(class_to_idx)}\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd5049e",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data\n",
    "\n",
    "Load the DataLoaders created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754485b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "‚úì Datasets loaded!\n",
      "Training samples: 60900\n",
      "Validation samples: 13050\n",
      "Test samples: 13050\n",
      "Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset class (same as in previous notebook)\n",
    "class SignLanguageDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None, class_to_idx=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name in os.listdir(data_path):\n",
    "            class_path = os.path.join(data_path, class_name)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "            \n",
    "            if class_to_idx and class_name in class_to_idx:\n",
    "                class_idx = class_to_idx[class_name]\n",
    "            else:\n",
    "                class_idx = len(set(self.labels))\n",
    "            \n",
    "            for img_name in os.listdir(class_path):\n",
    "                if img_name.endswith(('.jpg', '.png', '.jpeg')):\n",
    "                    self.images.append(os.path.join(class_path, img_name))\n",
    "                    self.labels.append(class_idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load data paths\n",
    "processed_data_path = os.path.join(project_root, 'data', 'processed')\n",
    "train_path = os.path.join(processed_data_path, 'train')\n",
    "val_path = os.path.join(processed_data_path, 'val')\n",
    "test_path = os.path.join(processed_data_path, 'test')\n",
    "\n",
    "# Create datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = SignLanguageDataset(train_path, transform=train_transform, class_to_idx=class_to_idx)\n",
    "val_dataset = SignLanguageDataset(val_path, transform=val_transform, class_to_idx=class_to_idx)\n",
    "test_dataset = SignLanguageDataset(test_path, transform=val_transform, class_to_idx=class_to_idx)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = int(config['batch_size'])\n",
    "# Set num_workers=0 for Windows to avoid hanging issues\n",
    "num_workers = 0  # int(config['num_workers'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                         num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n",
    "                       num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                        num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "print(f\"\\n‚úì Datasets loaded!\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9874cc",
   "metadata": {},
   "source": [
    "## 3. Build CNN Model Architecture\n",
    "\n",
    "We'll use a pre-trained ResNet18 model and fine-tune it for sign language recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b2928d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\nasir/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [02:47<00:00, 279kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL ARCHITECTURE\n",
      "============================================================\n",
      "Base model: ResNet18 (pretrained on ImageNet)\n",
      "Number of classes: 26\n",
      "Total parameters: 11,452,506\n",
      "Trainable parameters: 11,452,506\n",
      "============================================================\n",
      "\n",
      "Model structure:\n",
      "SignLanguageModel(\n",
      "  (model): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Sequential(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=26, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SignLanguageModel(nn.Module):\n",
    "    def __init__(self, num_classes=26, pretrained=True):\n",
    "        super(SignLanguageModel, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet18\n",
    "        self.model = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Modify the final fully connected layer\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Create model\n",
    "num_classes = len(class_to_idx)\n",
    "model = SignLanguageModel(num_classes=num_classes, pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Base model: ResNet18 (pretrained on ImageNet)\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nModel structure:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef578d75",
   "metadata": {},
   "source": [
    "## 4. Define Loss Function and Optimizer\n",
    "\n",
    "Set up training components including loss, optimizer, and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c4083e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING CONFIGURATION\n",
      "============================================================\n",
      "Loss function: CrossEntropyLoss\n",
      "Optimizer: Adam\n",
      "Learning rate: 0.001\n",
      "Weight decay: 1e-4\n",
      "LR scheduler: ReduceLROnPlateau\n",
      "  - Factor: 0.5\n",
      "  - Patience: 3\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = float(config['learning_rate'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                 factor=0.5, patience=3)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Loss function: CrossEntropyLoss\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Weight decay: 1e-4\")\n",
    "print(f\"LR scheduler: ReduceLROnPlateau\")\n",
    "print(f\"  - Factor: 0.5\")\n",
    "print(f\"  - Patience: 3\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309b4d3c",
   "metadata": {},
   "source": [
    "## 5. Training and Validation Functions\n",
    "\n",
    "Define functions for training and validation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b09c5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training and validation functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', \n",
    "                         'acc': f'{100*correct/total:.2f}%'})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}', \n",
    "                             'acc': f'{100*correct/total:.2f}%'})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"‚úì Training and validation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6507e10",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "Now let's train the model with early stopping and model checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23724eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Number of epochs: 50\n",
      "Early stopping patience: 10\n",
      "Checkpoint path: d:\\Projects\\RealTime-Sign-Language-Translator\\models\\checkpoints\n",
      "============================================================\n",
      "\n",
      "Epoch 1/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1904 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "num_epochs = int(config['num_epochs'])\n",
    "patience = int(config['early_stopping_patience'])\n",
    "checkpoint_path = config['checkpoint_path']\n",
    "model_path = config['model_path']\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "# Early stopping variables\n",
    "best_val_loss = float('inf')\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "epochs_no_improve = 0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Early stopping patience: {patience}\")\n",
    "print(f\"Checkpoint path: {checkpoint_path}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"  ‚úì Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}\")\n",
    "        best_val_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        epochs_no_improve = 0\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'history': history\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_path, 'best_model.pth'))\n",
    "        print(f\"  ‚úì Model checkpoint saved\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"  No improvement for {epochs_no_improve} epoch(s)\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"\\n‚ö† Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Save final model\n",
    "final_model_path = os.path.join(model_path, 'sign_language_model.pth')\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'class_to_idx': class_to_idx,\n",
    "    'idx_to_class': idx_to_class,\n",
    "    'model_config': {\n",
    "        'num_classes': num_classes,\n",
    "        'architecture': 'ResNet18'\n",
    "    }\n",
    "}, final_model_path)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total training time: {training_time/60:.2f} minutes\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best validation accuracy: {history['val_acc'][history['val_loss'].index(best_val_loss)]:.2f}%\")\n",
    "print(f\"Final model saved to: {final_model_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b473fc",
   "metadata": {},
   "source": [
    "## 7. Visualize Training History\n",
    "\n",
    "Plot training and validation metrics over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0d6e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(epochs_range, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "axes[0].plot(epochs_range, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(epochs_range, history['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(epochs_range, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate plot\n",
    "axes[2].plot(epochs_range, history['lr'], 'g-', linewidth=2)\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('Learning Rate', fontsize=12)\n",
    "axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "axes[2].set_yscale('log')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config['output_path'], 'visualizations', 'training_history.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Training history visualized and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf028d90",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model on Test Set\n",
    "\n",
    "Evaluate the trained model on the test set and compute detailed metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51050f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"Evaluate model and return predictions and labels.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc='Evaluating'):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_preds, test_labels = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_preds) * 100\n",
    "\n",
    "# Classification report\n",
    "class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "report = classification_report(test_labels, test_preds, target_names=class_names, digits=4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save metrics\n",
    "metrics_path = os.path.join(config['output_path'], 'metrics')\n",
    "os.makedirs(metrics_path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(metrics_path, 'classification_report.txt'), 'w') as f:\n",
    "    f.write(f\"Test Accuracy: {test_accuracy:.2f}%\\n\\n\")\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\n‚úì Metrics saved to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cba12ac",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix\n",
    "\n",
    "Visualize the confusion matrix to understand model performance across classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=14)\n",
    "plt.ylabel('True Label', fontsize=14)\n",
    "plt.title('Confusion Matrix - Sign Language Recognition', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config['output_path'], 'visualizations', 'confusion_matrix.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1) * 100\n",
    "\n",
    "# Display per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(\"=\"*60)\n",
    "for i, cls in enumerate(class_names):\n",
    "    print(f\"{cls:3s}: {class_accuracy[i]:6.2f}% ({cm.diagonal()[i]:4d}/{cm.sum(axis=1)[i]:4d})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find best and worst performing classes\n",
    "best_idx = np.argmax(class_accuracy)\n",
    "worst_idx = np.argmin(class_accuracy)\n",
    "\n",
    "print(f\"\\nBest performing class:  {class_names[best_idx]} ({class_accuracy[best_idx]:.2f}%)\")\n",
    "print(f\"Worst performing class: {class_names[worst_idx]} ({class_accuracy[worst_idx]:.2f}%)\")\n",
    "\n",
    "print(\"\\n‚úì Confusion matrix visualized and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b9c583",
   "metadata": {},
   "source": [
    "## 10. Visualize Sample Predictions\n",
    "\n",
    "Show model predictions on random test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af84b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor):\n",
    "    \"\"\"Denormalize image tensor for visualization.\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "def visualize_predictions(model, dataset, device, num_samples=12):\n",
    "    \"\"\"Visualize model predictions on random samples.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get random indices\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            image, true_label = dataset[idx]\n",
    "            \n",
    "            # Get prediction\n",
    "            image_batch = image.unsqueeze(0).to(device)\n",
    "            output = model(image_batch)\n",
    "            _, pred_label = torch.max(output, 1)\n",
    "            pred_label = pred_label.item()\n",
    "            \n",
    "            # Get confidence\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)\n",
    "            confidence = probs[0, pred_label].item() * 100\n",
    "            \n",
    "            # Denormalize and convert to numpy\n",
    "            img_denorm = denormalize(image)\n",
    "            img_np = img_denorm.permute(1, 2, 0).numpy()\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "            \n",
    "            # Get class names\n",
    "            true_class = idx_to_class[true_label]\n",
    "            pred_class = idx_to_class[pred_label]\n",
    "            \n",
    "            # Plot\n",
    "            axes[i].imshow(img_np)\n",
    "            axes[i].axis('off')\n",
    "            \n",
    "            # Color code: green for correct, red for incorrect\n",
    "            color = 'green' if true_label == pred_label else 'red'\n",
    "            title = f'True: {true_class}\\nPred: {pred_class} ({confidence:.1f}%)'\n",
    "            axes[i].set_title(title, fontsize=11, color=color, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Model Predictions on Test Set', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config['output_path'], 'visualizations', 'sample_predictions.png'), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions\n",
    "visualize_predictions(model, test_dataset, device, num_samples=12)\n",
    "\n",
    "print(\"‚úì Sample predictions visualized and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02dd772",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "Training and evaluation complete! Model is ready for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b2dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MODEL TRAINING AND EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä DATASET:\")\n",
    "print(f\"  Training samples:   {len(train_dataset):,}\")\n",
    "print(f\"  Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"  Test samples:       {len(test_dataset):,}\")\n",
    "print(f\"  Number of classes:  {num_classes}\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è  MODEL ARCHITECTURE:\")\n",
    "print(f\"  Base model:          ResNet18 (pretrained)\")\n",
    "print(f\"  Total parameters:    {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è  TRAINING CONFIGURATION:\")\n",
    "print(f\"  Optimizer:           Adam\")\n",
    "print(f\"  Learning rate:       {learning_rate}\")\n",
    "print(f\"  Batch size:          {batch_size}\")\n",
    "print(f\"  Epochs trained:      {len(history['train_loss'])}\")\n",
    "print(f\"  Training time:       {training_time/60:.2f} minutes\")\n",
    "\n",
    "print(\"\\nüìà PERFORMANCE METRICS:\")\n",
    "print(f\"  Best validation loss:     {best_val_loss:.4f}\")\n",
    "print(f\"  Best validation accuracy: {max(history['val_acc']):.2f}%\")\n",
    "print(f\"  Test accuracy:            {test_accuracy:.2f}%\")\n",
    "print(f\"  Best performing class:    {class_names[best_idx]} ({class_accuracy[best_idx]:.2f}%)\")\n",
    "print(f\"  Worst performing class:   {class_names[worst_idx]} ({class_accuracy[worst_idx]:.2f}%)\")\n",
    "\n",
    "print(\"\\nüíæ SAVED FILES:\")\n",
    "print(f\"  Model checkpoint:     {os.path.join(checkpoint_path, 'best_model.pth')}\")\n",
    "print(f\"  Final model:          {final_model_path}\")\n",
    "print(f\"  Training history:     {os.path.join(config['output_path'], 'visualizations', 'training_history.png')}\")\n",
    "print(f\"  Confusion matrix:     {os.path.join(config['output_path'], 'visualizations', 'confusion_matrix.png')}\")\n",
    "print(f\"  Classification report: {os.path.join(metrics_path, 'classification_report.txt')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. Test model on real-time webcam input\")\n",
    "print(\"2. Implement real-time sign language detection\")\n",
    "print(\"3. Create user interface for the application\")\n",
    "print(\"4. Deploy the model for production use\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save training history\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df.to_csv(os.path.join(metrics_path, 'training_history.csv'), index=False)\n",
    "print(f\"\\n‚úì Training history saved to: {os.path.join(metrics_path, 'training_history.csv')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
