{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Exploration\n",
    "## Real-Time Sign Language Translator\n",
    "\n",
    "This notebook covers:\n",
    "1. Environment setup and package installation\n",
    "2. Dataset download and setup\n",
    "3. Data exploration and visualization\n",
    "4. Class distribution analysis\n",
    "5. Sample image visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install Required Packages\n",
    "\n",
    "**IMPORTANT:** Run this cell first, then **RESTART THE KERNEL** before continuing.\n",
    "\n",
    "This fixes the NumPy-TensorFlow compatibility issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages with compatible versions\n",
    "# Fix NumPy version for TensorFlow compatibility\n",
    "!pip install \"numpy<1.24\" opencv-python pillow matplotlib seaborn kaggle -q\n",
    "\n",
    "print(\"[OK] All packages installed successfully!\")\n",
    "print(\"\")\n",
    "print(\"[!] IMPORTANT: Please RESTART the kernel now!\")\n",
    "print(\"    Kernel -> Restart Kernel\")\n",
    "print(\"\")\n",
    "print(\"[!] After restart, SKIP this cell and run from Section 1 onwards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"[OK] Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check CUDA Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"CUDA Available: {tf.test.is_built_with_cuda()}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Enable GPU memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"[OK] {len(gpus)} GPU(s) configured successfully!\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Kaggle API (for dataset download)\n",
    "\n",
    "**Instructions:**\n",
    "1. Go to https://www.kaggle.com/\n",
    "2. Click on your profile → Account → Create New API Token\n",
    "3. Download `kaggle.json`\n",
    "4. Place it in `C:\\Users\\nasir\\.kaggle\\kaggle.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Kaggle API is configured\n",
    "kaggle_dir = Path.home() / '.kaggle'\n",
    "kaggle_json = kaggle_dir / 'kaggle.json'\n",
    "\n",
    "if kaggle_json.exists():\n",
    "    print(\"[OK] Kaggle API configured!\")\n",
    "else:\n",
    "    print(\"[X] Kaggle API not configured. Please follow the instructions above.\")\n",
    "    print(f\"Expected location: {kaggle_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download ASL Alphabet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset using Kaggle API\n",
    "!kaggle datasets download -d grassknoted/asl-alphabet -p ../data/raw/ --unzip\n",
    "\n",
    "print(\"[OK] Dataset downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_dir = Path('../data/raw/asl_alphabet_train/asl_alphabet_train')\n",
    "\n",
    "# Get class names\n",
    "class_names = sorted([d.name for d in data_dir.iterdir() if d.is_dir()])\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"\\nClass names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images per class\n",
    "class_counts = {}\n",
    "for class_name in class_names:\n",
    "    class_path = data_dir / class_name\n",
    "    count = len(list(class_path.glob('*.jpg')))\n",
    "    class_counts[class_name] = count\n",
    "\n",
    "# Create DataFrame\n",
    "df_counts = pd.DataFrame(list(class_counts.items()), columns=['Class', 'Count'])\n",
    "df_counts = df_counts.sort_values('Count', ascending=False)\n",
    "\n",
    "print(df_counts)\n",
    "print(f\"\\nTotal images: {df_counts['Count'].sum():,}\")\n",
    "print(f\"Average images per class: {df_counts['Count'].mean():.0f}\")\n",
    "print(f\"Min images: {df_counts['Count'].min()}\")\n",
    "print(f\"Max images: {df_counts['Count'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class distribution\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(df_counts['Class'], df_counts['Count'], color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Class', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Images', fontsize=12, fontweight='bold')\n",
    "plt.title('ASL Alphabet Dataset - Class Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each class\n",
    "fig, axes = plt.subplots(5, 6, figsize=(18, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, class_name in enumerate(class_names[:30]):\n",
    "    class_path = data_dir / class_name\n",
    "    # Get first image\n",
    "    img_path = list(class_path.glob('*.jpg'))[0]\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"Class: {class_name}\", fontsize=10, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Images from Each Class', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyze Image Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random images to check properties\n",
    "sample_images = []\n",
    "for class_name in class_names[:5]:\n",
    "    class_path = data_dir / class_name\n",
    "    img_path = list(class_path.glob('*.jpg'))[0]\n",
    "    img = cv2.imread(str(img_path))\n",
    "    sample_images.append(img)\n",
    "\n",
    "# Check dimensions\n",
    "print(\"Image dimensions:\")\n",
    "for i, img in enumerate(sample_images):\n",
    "    print(f\"  Class {class_names[i]}: {img.shape}\")\n",
    "\n",
    "# Check data type and range\n",
    "print(f\"\\nData type: {sample_images[0].dtype}\")\n",
    "print(f\"Pixel value range: [{sample_images[0].min()}, {sample_images[0].max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Display Multiple Samples from One Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show variations within a single class\n",
    "selected_class = 'A'  # Change this to any class\n",
    "class_path = data_dir / selected_class\n",
    "image_paths = list(class_path.glob('*.jpg'))[:12]\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, img_path in enumerate(image_paths):\n",
    "    img = Image.open(img_path)\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(f'Variations in Class \"{selected_class}\"', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "- Dataset contains 29 classes (A-Z + space, delete, nothing)\n",
    "- ~87,000 total images\n",
    "- Images are 200x200 pixels, RGB\n",
    "- Balanced class distribution\n",
    "\n",
    "### Next Steps:\n",
    "1. **Data Preprocessing** (Notebook 02)\n",
    "   - Resize images to 224x224 for transfer learning\n",
    "   - Normalize pixel values\n",
    "   - Create train/val/test splits\n",
    "   \n",
    "2. **Data Augmentation** (Notebook 02)\n",
    "   - Apply transformations to increase dataset size\n",
    "   - Improve model generalization\n",
    "   \n",
    "3. **Model Training** (Notebook 03)\n",
    "   - Build and train CNN model\n",
    "   - Use transfer learning (MobileNetV2/EfficientNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[OK] Data exploration complete!\")\n",
    "print(\"[>>] Ready to proceed to data preprocessing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}