{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìä Model Evaluation - Performance Analysis\n",
                "\n",
                "This notebook performs comprehensive evaluation of the trained sign language classifier.\n",
                "\n",
                "## Objectives\n",
                "- Load trained model and test data\n",
                "- Generate predictions on test set\n",
                "- Calculate detailed performance metrics\n",
                "- Create confusion matrix visualization\n",
                "- Analyze per-class performance\n",
                "- Identify misclassifications\n",
                "- Generate classification report\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pickle\n",
                "import json\n",
                "from datetime import datetime\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# TensorFlow and Keras\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "\n",
                "# Scikit-learn metrics\n",
                "from sklearn.metrics import (\n",
                "    classification_report, confusion_matrix, accuracy_score,\n",
                "    precision_score, recall_score, f1_score,\n",
                "    roc_auc_score, cohen_kappa_score\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model and Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load test data\n",
                "PROCESSED_DIR = 'data/processed'\n",
                "MODELS_DIR = 'models/saved_models'\n",
                "\n",
                "X_test = np.load(os.path.join(PROCESSED_DIR, 'X_test.npy'))\n",
                "y_test = np.load(os.path.join(PROCESSED_DIR, 'y_test.npy'))\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"TEST DATA LOADED\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Test samples: {X_test.shape[0]}\")\n",
                "print(f\"Features: {X_test.shape[1]}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load label encoder\n",
                "with open('models/label_encoder.pkl', 'rb') as f:\n",
                "    label_encoder = pickle.load(f)\n",
                "\n",
                "class_names = label_encoder.classes_\n",
                "num_classes = len(class_names)\n",
                "\n",
                "print(f\"\\nClasses: {class_names}\")\n",
                "print(f\"Number of classes: {num_classes}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load trained model\n",
                "model = keras.models.load_model(os.path.join(MODELS_DIR, 'best_model.keras'))\n",
                "\n",
                "print(\"\\n‚úÖ Model loaded successfully\")\n",
                "print(f\"   Model: {model.name}\")\n",
                "print(f\"   Total parameters: {model.count_params():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Generate Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate predictions\n",
                "print(\"\\nGenerating predictions...\")\n",
                "y_pred_proba = model.predict(X_test, verbose=0)\n",
                "y_pred = np.argmax(y_pred_proba, axis=1)\n",
                "\n",
                "print(\"‚úÖ Predictions generated\")\n",
                "print(f\"   Prediction shape: {y_pred.shape}\")\n",
                "print(f\"   Probability shape: {y_pred_proba.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Overall Performance Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate metrics\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "precision = precision_score(y_test, y_pred, average='weighted')\n",
                "recall = recall_score(y_test, y_pred, average='weighted')\n",
                "f1 = f1_score(y_test, y_pred, average='weighted')\n",
                "kappa = cohen_kappa_score(y_test, y_pred)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"OVERALL PERFORMANCE METRICS\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Accuracy:  {accuracy*100:.2f}%\")\n",
                "print(f\"Precision: {precision*100:.2f}%\")\n",
                "print(f\"Recall:    {recall*100:.2f}%\")\n",
                "print(f\"F1-Score:  {f1*100:.2f}%\")\n",
                "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate confusion matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "\n",
                "# Plot confusion matrix\n",
                "plt.figure(figsize=(12, 10))\n",
                "sns.heatmap(\n",
                "    cm, \n",
                "    annot=True, \n",
                "    fmt='d', \n",
                "    cmap='Blues',\n",
                "    xticklabels=class_names,\n",
                "    yticklabels=class_names,\n",
                "    cbar_kws={'label': 'Count'},\n",
                "    linewidths=0.5,\n",
                "    linecolor='gray'\n",
                ")\n",
                "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
                "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
                "plt.title('Confusion Matrix - Test Set', fontsize=14, fontweight='bold', pad=20)\n",
                "plt.tight_layout()\n",
                "plt.savefig('outputs/visualizations/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úÖ Confusion matrix saved to outputs/visualizations/confusion_matrix.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Normalized confusion matrix\n",
                "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "sns.heatmap(\n",
                "    cm_normalized, \n",
                "    annot=True, \n",
                "    fmt='.2f', \n",
                "    cmap='YlOrRd',\n",
                "    xticklabels=class_names,\n",
                "    yticklabels=class_names,\n",
                "    cbar_kws={'label': 'Proportion'},\n",
                "    linewidths=0.5,\n",
                "    linecolor='gray',\n",
                "    vmin=0,\n",
                "    vmax=1\n",
                ")\n",
                "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
                "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
                "plt.title('Normalized Confusion Matrix - Test Set', fontsize=14, fontweight='bold', pad=20)\n",
                "plt.tight_layout()\n",
                "plt.savefig('outputs/visualizations/confusion_matrix_normalized.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úÖ Normalized confusion matrix saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Per-Class Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate classification report\n",
                "report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)\n",
                "\n",
                "# Convert to DataFrame\n",
                "report_df = pd.DataFrame(report).transpose()\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"CLASSIFICATION REPORT\")\n",
                "print(\"=\"*60)\n",
                "print(classification_report(y_test, y_pred, target_names=class_names))\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize per-class metrics\n",
                "metrics_df = report_df.iloc[:-3, :3]  # Exclude avg rows, keep precision/recall/f1\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "# Precision\n",
                "axes[0].bar(metrics_df.index, metrics_df['precision'], color='skyblue', edgecolor='navy')\n",
                "axes[0].set_xlabel('Class', fontsize=11, fontweight='bold')\n",
                "axes[0].set_ylabel('Precision', fontsize=11, fontweight='bold')\n",
                "axes[0].set_title('Precision by Class', fontsize=12, fontweight='bold')\n",
                "axes[0].set_ylim([0, 1.1])\n",
                "axes[0].grid(axis='y', alpha=0.3)\n",
                "axes[0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "# Recall\n",
                "axes[1].bar(metrics_df.index, metrics_df['recall'], color='lightgreen', edgecolor='darkgreen')\n",
                "axes[1].set_xlabel('Class', fontsize=11, fontweight='bold')\n",
                "axes[1].set_ylabel('Recall', fontsize=11, fontweight='bold')\n",
                "axes[1].set_title('Recall by Class', fontsize=12, fontweight='bold')\n",
                "axes[1].set_ylim([0, 1.1])\n",
                "axes[1].grid(axis='y', alpha=0.3)\n",
                "axes[1].tick_params(axis='x', rotation=45)\n",
                "\n",
                "# F1-Score\n",
                "axes[2].bar(metrics_df.index, metrics_df['f1-score'], color='lightcoral', edgecolor='darkred')\n",
                "axes[2].set_xlabel('Class', fontsize=11, fontweight='bold')\n",
                "axes[2].set_ylabel('F1-Score', fontsize=11, fontweight='bold')\n",
                "axes[2].set_title('F1-Score by Class', fontsize=12, fontweight='bold')\n",
                "axes[2].set_ylim([0, 1.1])\n",
                "axes[2].grid(axis='y', alpha=0.3)\n",
                "axes[2].tick_params(axis='x', rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('outputs/visualizations/per_class_metrics.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Prediction Confidence Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get prediction confidences\n",
                "confidences = np.max(y_pred_proba, axis=1)\n",
                "\n",
                "# Separate correct and incorrect predictions\n",
                "correct_mask = (y_pred == y_test)\n",
                "correct_confidences = confidences[correct_mask]\n",
                "incorrect_confidences = confidences[~correct_mask]\n",
                "\n",
                "print(\"\\nPrediction Confidence Analysis:\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Correct predictions:   {len(correct_confidences)} ({len(correct_confidences)/len(y_test)*100:.1f}%)\")\n",
                "print(f\"  Mean confidence: {correct_confidences.mean():.4f}\")\n",
                "print(f\"  Std confidence:  {correct_confidences.std():.4f}\")\n",
                "print(f\"\\nIncorrect predictions: {len(incorrect_confidences)} ({len(incorrect_confidences)/len(y_test)*100:.1f}%)\")\n",
                "if len(incorrect_confidences) > 0:\n",
                "    print(f\"  Mean confidence: {incorrect_confidences.mean():.4f}\")\n",
                "    print(f\"  Std confidence:  {incorrect_confidences.std():.4f}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize confidence distributions\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.hist(correct_confidences, bins=30, color='green', alpha=0.7, edgecolor='black')\n",
                "plt.xlabel('Confidence', fontsize=11)\n",
                "plt.ylabel('Frequency', fontsize=11)\n",
                "plt.title('Confidence Distribution - Correct Predictions', fontsize=12, fontweight='bold')\n",
                "plt.axvline(correct_confidences.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {correct_confidences.mean():.3f}')\n",
                "plt.legend()\n",
                "plt.grid(alpha=0.3)\n",
                "\n",
                "if len(incorrect_confidences) > 0:\n",
                "    plt.subplot(1, 2, 2)\n",
                "    plt.hist(incorrect_confidences, bins=30, color='red', alpha=0.7, edgecolor='black')\n",
                "    plt.xlabel('Confidence', fontsize=11)\n",
                "    plt.ylabel('Frequency', fontsize=11)\n",
                "    plt.title('Confidence Distribution - Incorrect Predictions', fontsize=12, fontweight='bold')\n",
                "    plt.axvline(incorrect_confidences.mean(), color='blue', linestyle='--', linewidth=2, label=f'Mean: {incorrect_confidences.mean():.3f}')\n",
                "    plt.legend()\n",
                "    plt.grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('outputs/visualizations/confidence_distribution.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Misclassification Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find misclassified samples\n",
                "misclassified_indices = np.where(y_pred != y_test)[0]\n",
                "\n",
                "if len(misclassified_indices) > 0:\n",
                "    print(f\"\\nMisclassified Samples: {len(misclassified_indices)}\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    # Show first 10 misclassifications\n",
                "    print(\"\\nFirst 10 Misclassifications:\")\n",
                "    print(f\"{'Index':<8} {'True':<10} {'Predicted':<10} {'Confidence':<12}\")\n",
                "    print(\"-\" * 60)\n",
                "    \n",
                "    for idx in misclassified_indices[:10]:\n",
                "        true_label = class_names[y_test[idx]]\n",
                "        pred_label = class_names[y_pred[idx]]\n",
                "        confidence = confidences[idx]\n",
                "        print(f\"{idx:<8} {true_label:<10} {pred_label:<10} {confidence:<12.4f}\")\n",
                "else:\n",
                "    print(\"\\nüéâ Perfect classification! No misclassifications found!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Evaluation Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create evaluation metadata\n",
                "evaluation_metadata = {\n",
                "    'evaluation_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
                "    'test_samples': int(len(y_test)),\n",
                "    'num_classes': int(num_classes),\n",
                "    'classes': class_names.tolist(),\n",
                "    'overall_metrics': {\n",
                "        'accuracy': float(accuracy),\n",
                "        'precision': float(precision),\n",
                "        'recall': float(recall),\n",
                "        'f1_score': float(f1),\n",
                "        'cohen_kappa': float(kappa)\n",
                "    },\n",
                "    'per_class_metrics': report,\n",
                "    'misclassified_count': int(len(misclassified_indices)),\n",
                "    'correct_predictions': int(len(correct_confidences)),\n",
                "    'mean_confidence_correct': float(correct_confidences.mean()),\n",
                "    'mean_confidence_incorrect': float(incorrect_confidences.mean()) if len(incorrect_confidences) > 0 else None\n",
                "}\n",
                "\n",
                "# Save metadata\n",
                "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "with open(f'outputs/metrics/evaluation_results_{timestamp}.json', 'w') as f:\n",
                "    json.dump(evaluation_metadata, f, indent=4)\n",
                "\n",
                "print(\"\\n‚úÖ Evaluation results saved!\")\n",
                "print(f\"   File: outputs/metrics/evaluation_results_{timestamp}.json\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save confusion matrix as CSV\n",
                "cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
                "cm_df.to_csv(f'outputs/metrics/confusion_matrix_{timestamp}.csv')\n",
                "\n",
                "print(\"‚úÖ Confusion matrix saved as CSV\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Final Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"EVALUATION SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nüìä Test Set Performance:\")\n",
                "print(f\"   Samples evaluated: {len(y_test)}\")\n",
                "print(f\"   Accuracy: {accuracy*100:.2f}%\")\n",
                "print(f\"   Precision: {precision*100:.2f}%\")\n",
                "print(f\"   Recall: {recall*100:.2f}%\")\n",
                "print(f\"   F1-Score: {f1*100:.2f}%\")\n",
                "\n",
                "print(f\"\\n‚úÖ Correct predictions: {len(correct_confidences)} ({len(correct_confidences)/len(y_test)*100:.1f}%)\")\n",
                "print(f\"‚ùå Incorrect predictions: {len(misclassified_indices)} ({len(misclassified_indices)/len(y_test)*100:.1f}%)\")\n",
                "\n",
                "print(f\"\\nüìà Best performing class: {metrics_df['f1-score'].idxmax()} (F1: {metrics_df['f1-score'].max():.3f})\")\n",
                "print(f\"üìâ Worst performing class: {metrics_df['f1-score'].idxmin()} (F1: {metrics_df['f1-score'].min():.3f})\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ EVALUATION COMPLETE!\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\nNext: Proceed to 06_realtime_detection.ipynb for live testing\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéØ Summary\n",
                "\n",
                "Model evaluation completed successfully!\n",
                "\n",
                "### What Was Done:\n",
                "- ‚úÖ Generated predictions on test set\n",
                "- ‚úÖ Calculated comprehensive performance metrics\n",
                "- ‚úÖ Created confusion matrices (raw and normalized)\n",
                "- ‚úÖ Analyzed per-class performance\n",
                "- ‚úÖ Examined prediction confidence\n",
                "- ‚úÖ Identified misclassifications\n",
                "- ‚úÖ Saved evaluation results\n",
                "\n",
                "### What's Next?\n",
                "Proceed to **06_realtime_detection.ipynb** to:\n",
                "- Test the model with live webcam feed\n",
                "- Perform real-time sign language translation\n",
                "- Evaluate real-world performance\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}